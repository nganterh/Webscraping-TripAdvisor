{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>WebScraping TripAdvisor</h1>\n",
    "\n",
    "---\n",
    "\n",
    "El código a continuación tiene por objetivo extraer la **[información solicitada](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"Word en Google Drive\")**, desde la página de **[TripAdvisor](https://www.tripadvisor.cl/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html \"Web TripAdvisor\")** para Ximena. La siguiente celda sólo cumple con el propósito de **silenciar las posibles advertencias** que pudieran levantarse al correr el código, pero no aportan mayormente a la comprensión del proceso por parte del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda anterior asegurará que no se desplieguen advertencias innecesarias para la correcta comprensión y lectura de este informe. A continuación se darán las **instrucciones para instalar las librerías** necesarias para correr el código, cuestión que requiere de un comando para ello, por lo que las instrucciones se despliegan como impresión de una celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f'Si es la primera vez que corre este programa, por favor abra la terminal PowerShell de Anaconda' +\n",
    "      f' e ingrese el siguiente comando: \"\\033[4mpip install -r {os.getcwd()}\\\\requirements.txt\\033[4m\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte fundamental de todo programa, corresponde a la **importación de librerías de Python**. Si acaso hubiera errores en esta primera celda, se aconseja contactar a Nicolás Ganter a su correo: nicolas@ganter.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from distributed import Client, LocalCluster, progress\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del código, hay ciertas **variables que es preferible tener en especial consideración**. Entre ellas, encontramos la ubicación del *driver* para *Selenium* que permitirá lanzar una instancia de *Firefox* para navegar la página y extraer los enlaces requeridos en la primera etapa de *webcrawling*. Si aún no ha instalado el driver, acceda a este **[link de descarga](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"geckodriver download link\")**, extraiga el paquete y mueva los documentos a la carpeta de binarios de las librerías de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geckodriver_path = r'C:\\Users\\nicol\\anaconda3\\Library\\bin\\geckodriver'\n",
    "time_id = datetime.today().strftime('%Y%m%d')\n",
    "basic_url = 'https://www.tripadvisor.cl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dask raising instructions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cd C:\\Users\\nicol\\Proyectos\\GitHub\\Webscraping-TripAdvisor \n",
      " dask-worker tcp://192.168.1.105:8786 --preload worker_setup.py \n",
      " dask-worker --nprocs 6 --nthreads 1 tcp://192.168.1.105:8786 --preload worker_setup.py\n"
     ]
    }
   ],
   "source": [
    "print(f' cd {os.getcwd()}', '\\n', 'dask-worker tcp://192.168.1.105:8786 --preload worker_setup.py', '\\n',\n",
    "     'dask-worker --nprocs 6 --nthreads 1 tcp://192.168.1.105:8786 --preload worker_setup.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se creará nuestro clúster para trabajar en forma distribuída cada tarea a realizarse. En particular, utilizaremos un *LocalCluster* para levantar en una máquina cierta cantidad de *workers* con determinada configuración pre-cargada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:56664</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>17.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:56664' processes=6 threads=6, memory=17.02 GB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCluster(threads_per_worker=1, preload='worker_setup.py')\n",
    "client = Client(cluster)\n",
    "#client = Client('192.168.1.105:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los restaurantes</h2>\n",
    "La siguiente celda se encargará de la **extracción de los enlaces** asociados a cada restaurante en las páginas especificadas mediante el enlace de la segunda línea. En este caso, se extraerán los restaurantes de Santiago de Chile. Al final de la celda se imprime la cantidad de restaurantes extraídos, la cantidad de restaurantes disponibles según la página, y el porcentaje capturado por el programa. Nótese que el proceso toma algo así como 10 minutos, por lo que se utilizará un atajo mediante *pickles* (estructura de datos propia de este lenguaje de programación) y se especificará la fecha de captura de la información asociada a éste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4830_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Se obtuvieron 4830 restaurantes de 4852 lo que corresponde a una extracción del 99.55%\n",
      "Este proceso tomó 12.02 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url = basic_url + '/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html'\n",
    "info = utils.info_restaurants(url, geckodriver_path)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dict_pickles = utils.check_files(dir_files=cwd, keyword='urls')\n",
    "\n",
    "if len(dict_pickles) == 0:\n",
    "    urls = utils.gen_pickle(url, geckodriver_path, info['pages'], basic_url, time_id)\n",
    "\n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_pickles)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        urls = pickle.load(file)\n",
    "    \n",
    "print('Se obtuvieron {} restaurantes de {} lo que corresponde a una extracción del {}%'\n",
    "      .format(len(urls), info['max_restaurants'], round(len(urls) / info['max_restaurants'] * 100, 2)))\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los restaurantes</h2>\n",
    "Con esto concluye la parte más compleja y crítica de la recopilación de enlaces para los restaurantes. No obstante esta tarea continúa luego a nivel de comentarios, **a continuación se procederá a extraer la información solicitada** para cada uno de los restaurantes en la lista. Dado que se utilizan estrategias de computación paralela, no es posible observar el avance, sino abriendo el *Dashboard* cuyo link se encuentra bajo la cuarta celda del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210215_dataframe_of_788_scraped_reviews.pickle extraído el 15 de febrero del 2021.\n",
      "Este proceso tomó 0.02 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_dataframes = utils.check_files(dir_files=cwd, keyword='dataframe')\n",
    "\n",
    "if len(dict_dataframes) == 0:\n",
    "    futures = [client.submit(utils.get_restaurant, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_structure = {'id':[], 'Nombre restaurante':[], 'Promedio de calificaciones':[],\n",
    "                      'N° de opiniones':[], 'Calificación de viajeros por categoría':[],\n",
    "                      'Toman medidas de seguridad':[], 'Rankings':[],\n",
    "                      'Tipo de comida y servicios':[], 'url':[]}\n",
    "    \n",
    "    df_restaurants = utils.build_dataframe(dict_structure, results, time_id)\n",
    "    df_restaurants.to_pickle(f'{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle')\n",
    "    print(f'Se guardó \"{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_dataframes)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        df_restaurants = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los comentarios</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4321_review_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Este proceso tomó 0.02 segundos en correr. Se dispone aproximadamente de 43210 comentarios para extraer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='review_urls')\n",
    "\n",
    "if len(dict_files) == 0:\n",
    "    futures = [client.submit(utils.review_urls, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_reviews = {key:value for key, value in results if isinstance(value, list)}\n",
    "    n_reviews = len(dict_reviews.values())\n",
    "    \n",
    "    with open(f'{time_id}_{n_reviews}_review_urls.pickle', 'wb') as file:\n",
    "        pickle.dump(dict_reviews, file)\n",
    "    \n",
    "    print(f'Se guardó \"{time_id}_{n_reviews}_review_urls.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_files)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        dict_reviews = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "      'Se dispone aproximadamente de {} comentarios para extraer.\\n'.format(len(dict_reviews.values())*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation of urls</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='scraped_reviews')\n",
    "url_reviews = utils.prepare_urls(dict_reviews)\n",
    "dict_chunks = {}\n",
    "\n",
    "chunks = utils.gen_chunks(url_reviews, None, 100)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    dict_chunks['chunk{:02d}'.format(i+1)] = chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los comentarios</h2>\n",
    "\n",
    "A continuación se extraerán los comentarios, con un enfoque en chunks, para evitar problemas de deserialización en Dask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "try:\n",
    "    dir_pickles = os.mkdir(str(cwd) + r'\\Pickles\\Chunks')\n",
    "except:\n",
    "    dir_pickles = f'{str(cwd)}\\\\Pickles\\\\Chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(dict_chunks):\n",
    "\n",
    "    if i < 5:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        url_reviews = dict_chunks[chunk]\n",
    "\n",
    "        futures = [client.submit(utils.get_reviews, url) for url in url_reviews]\n",
    "        results = client.gather(futures)\n",
    "\n",
    "        dict_structure = {'id':[], 'date_review':[], 'comments':[], 'date_stayed':[], 'response_body':[],\n",
    "                          'user_name':[], 'user_reviews':[], 'useful_votes':[]}\n",
    "\n",
    "        df_reviews = utils.build_dataframe(dict_structure, results, time_id)\n",
    "        df_pathname = (f'{time_id}_{chunk}_dataframe_with_' +\n",
    "                       f'{df_reviews.shape[0]}_reviews.pickle')\n",
    "\n",
    "        df_reviews.to_pickle(f'{dir_pickles}\\\\{df_pathname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generación de tablas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants.xlsx')\n",
    "#df_reviews.to_excel(f'{time_id}_excel_with_{df_reviews.shape[0]}_reviews.xlsx')\n",
    "#stop = time.time()\n",
    "#\n",
    "#print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "#      'Se extrajeron {} comentarios.\\n'.format(df_reviews.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#\n",
    "#urls_test = url_reviews\n",
    "#\n",
    "#futures = [client.submit(utils.get_reviews, url) for url in urls_test]\n",
    "#results = client.gather(futures)\n",
    "#\n",
    "#stop = time.time()\n",
    "#\n",
    "#dict_structure = {'id':[], 'date_review':[], 'comments':[], 'date_stayed':[], 'response_body':[],\n",
    "#                  'user_name':[], 'user_reviews':[], 'useful_votes':[]}\n",
    "#\n",
    "#df_reviews = utils.build_dataframe(dict_structure, results, time_id)\n",
    "#\n",
    "#print(f'Este proceso tomó {round(stop-start, 2)} segundos')\n",
    "#display(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reviews.to_excel('test01.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random as rd\n",
    "#\n",
    "#def simple_sum():\n",
    "#    return rd.randint(0, 10) * rd.randint(0, 10)\n",
    "#\n",
    "#futures = [client.submit(simple_sum) for simple in range(100000)]\n",
    "#results = client.gather(futures)\n",
    "#\n",
    "#print(simple_sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extraer comunas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import googlemaps\n",
    "#\n",
    "#gmaps = googlemaps.Client(key='AIzaSyAv9kBNSqEznAwQ3nhnb1A6GZPlxJteLE8')\n",
    "#\n",
    "#def get_location(address):\n",
    "#    geocode = dict(*gmaps.geocode([address]))['address_components']\n",
    "#    location = str(geocode[3]['long_name'])\n",
    "#    \n",
    "#    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants = pd.read_pickle('20210205_dataframe_of_4830_restaurants.pickle')\n",
    "#addresses = df_restaurants['Dirección'].to_list()\n",
    "#locations = []\n",
    "#\n",
    "#for address in tqdm(addresses):\n",
    "#    try:\n",
    "#        location = get_location(address)\n",
    "#        locations.append(location)\n",
    "#        \n",
    "#    except:\n",
    "#        locations.append(address)\n",
    "        \n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants['Comuna'] = locations\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants_with_locations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = url_reviews[7]\n",
    "#\n",
    "#print(url['scraping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
