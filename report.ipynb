{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>WebScraping TripAdvisor</h1>\n",
    "\n",
    "---\n",
    "\n",
    "El código a continuación tiene por objetivo extraer la **[información solicitada](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"Word en Google Drive\")**, desde la página de **[TripAdvisor](https://www.tripadvisor.cl/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html \"Web TripAdvisor\")** para Ximena. La siguiente celda sólo cumple con el propósito de **silenciar las posibles advertencias** que pudieran levantarse al correr el código, pero no aportan mayormente a la comprensión del proceso por parte del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda anterior asegurará que no se desplieguen advertencias innecesarias para la correcta comprensión y lectura de este informe. A continuación se darán las **instrucciones para instalar las librerías** necesarias para correr el código, cuestión que requiere de un comando para ello, por lo que las instrucciones se despliegan como impresión de una celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si es la primera vez que corre este programa, por favor abra la terminal PowerShell de Anaconda e ingrese el siguiente comando: \"\u001b[4mpip install -r C:\\Users\\nicol\\Proyectos\\GitHub\\Webscraping-TripAdvisor\\requirements.txt\u001b[4m\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f'Si es la primera vez que corre este programa, por favor abra la terminal PowerShell de Anaconda' +\n",
    "      f' e ingrese el siguiente comando: \"\\033[4mpip install -r {os.getcwd()}\\\\requirements.txt\\033[4m\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte fundamental de todo programa, corresponde a la **importación de librerías de Python**. Si acaso hubiera errores en esta primera celda, se aconseja contactar a Nicolás Ganter a su correo: nicolas@ganter.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del código, hay ciertas **variables que es preferible tener en especial consideración**. Entre ellas, encontramos la ubicación del *driver* para *Selenium* que permitirá lanzar una instancia de *Firefox* para navegar la página y extraer los enlaces requeridos en la primera etapa de *webcrawling*. Si aún no ha instalado el driver, acceda a este **[link de descarga](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"geckodriver download link\")**, extraiga el paquete y mueva los documentos a la carpeta de binarios de las librerías de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geckodriver_path = r'C:\\Users\\nicol\\anaconda3\\Library\\bin\\geckodriver'\n",
    "time_id = datetime.today().strftime('%Y%m%d')\n",
    "basic_url = 'https://www.tripadvisor.cl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:64807</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>17.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:64807' processes=3 threads=6, memory=17.02 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los restaurantes</h2>\n",
    "La siguiente celda se encargará de la **extracción de los enlaces** asociados a cada restaurante en las páginas especificadas mediante el enlace de la segunda línea. En este caso, se extraerán los restaurantes de Santiago de Chile. Al final de la celda se imprime la cantidad de restaurantes extraídos, la cantidad de restaurantes disponibles según la página, y el porcentaje capturado por el programa. Nótese que el proceso toma algo así como 10 minutos, por lo que se utilizará un atajo mediante *pickles* (estructura de datos propia de este lenguaje de programación) y se especificará la fecha de captura de la información asociada a éste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4830_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Se obtuvieron 4830 restaurantes de 4847 lo que corresponde a una extracción del 99.65%\n",
      "Este proceso tomó 13.8 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url = basic_url + '/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html'\n",
    "info = utils.info_restaurants(url, geckodriver_path)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dict_pickles = utils.check_files(dir_files=cwd, keyword='urls')\n",
    "\n",
    "if len(dict_pickles) == 0:\n",
    "    urls = utils.gen_pickle(url, geckodriver_path, info['pages'], basic_url, time_id)\n",
    "\n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_pickles)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        urls = pickle.load(file)\n",
    "    \n",
    "print('Se obtuvieron {} restaurantes de {} lo que corresponde a una extracción del {}%'\n",
    "      .format(len(urls), info['max_restaurants'], round(len(urls) / info['max_restaurants'] * 100, 2)))\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los restaurantes</h2>\n",
    "Con esto concluye la parte más compleja y crítica de la recopilación de enlaces para los restaurantes. No obstante esta tarea continúa luego a nivel de comentarios, **a continuación se procederá a extraer la información solicitada** para cada uno de los restaurantes en la lista. Dado que se utilizan estrategias de computación paralela, no es posible observar el avance, sino abriendo el *Dashboard* cuyo link se encuentra bajo la cuarta celda del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210206_dataframe_of_205860_scraped_reviews.pickle extraído el 06 de febrero del 2021.\n",
      "Este proceso tomó 0.36 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_dataframes = utils.check_files(dir_files=cwd, keyword='dataframe')\n",
    "\n",
    "if len(dict_dataframes) == 0:\n",
    "    futures = [client.submit(utils.get_restaurant, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_structure = {'id':[], 'Nombre restaurante':[], 'Promedio de calificaciones':[],\n",
    "                      'N° de opiniones':[], 'Calificación de viajeros por categoría':[],\n",
    "                      'Toman medidas de seguridad':[], 'Rankings':[],\n",
    "                      'Tipo de comida y servicios':[], 'url':[]}\n",
    "    \n",
    "    df_restaurants = utils.build_dataframe(dict_structure, results, time_id)\n",
    "    df_restaurants.to_pickle(f'{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle')\n",
    "    print(f'Se guardó \"{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_dataframes)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        df_restaurants = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los comentarios</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4321_review_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Este proceso tomó 0.04 segundos en correr. Se dispone aproximadamente de 43210 comentarios para extraer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='review_urls')\n",
    "\n",
    "if len(dict_files) == 0:\n",
    "    futures = [client.submit(utils.review_urls, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_reviews = {key:value for key, value in results if isinstance(value, list)}\n",
    "    n_reviews = len(dict_reviews.values())\n",
    "    \n",
    "    with open(f'{time_id}_{n_reviews}_review_urls.pickle', 'wb') as file:\n",
    "        pickle.dump(dict_reviews, file)\n",
    "    \n",
    "    print(f'Se guardó \"{time_id}_{n_reviews}_review_urls.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_files)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        dict_reviews = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "      'Se dispone aproximadamente de {} comentarios para extraer.\\n'.format(len(dict_reviews.values())*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los comentarios</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210206_dataframe_of_205860_scraped_reviews.pickle extraído el 06 de febrero del 2021.\n",
      "Este proceso tomó 0.36 segundos en correr. Se extrajeron 205860 comentarios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='scraped_reviews')\n",
    "url_reviews = utils.prepare_urls(dict_reviews)\n",
    "\n",
    "if len(dict_files) == 0:\n",
    "    futures = [client.submit(utils.get_reviews, url) for url in url_reviews]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_structure = {'id':[], 'date_review':[], 'comments':[], 'date_stayed':[], 'response_body':[],\n",
    "                      'user_name':[], 'user_reviews':[], 'useful_votes':[]}\n",
    "\n",
    "    df_reviews = utils.build_dataframe(dict_structure, results, time_id)\n",
    "    df_pathname = f'{time_id}_dataframe_of_{df_reviews.shape[0]}_scraped_reviews.pickle'\n",
    "\n",
    "    df_reviews.to_pickle(df_pathname)\n",
    "    print(f'Se guardó \"{df_pathname}\" en \"{os.getcwd()}\"')\n",
    "        \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_files)\n",
    "    df_reviews = pd.read_pickle(last_pickle)\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "      'Se extrajeron {} comentarios.\\n'.format(df_reviews.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generación de tablas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants.xlsx')\n",
    "#df_reviews.to_excel(f'{time_id}_excel_with_{df_reviews.shape[0]}_reviews.xlsx')\n",
    "#stop = time.time()\n",
    "#\n",
    "#print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "#      'Se extrajeron {} comentarios.\\n'.format(df_reviews.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extraer comunas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import googlemaps\n",
    "#\n",
    "#gmaps = googlemaps.Client(key='AIzaSyAv9kBNSqEznAwQ3nhnb1A6GZPlxJteLE8')\n",
    "#\n",
    "#def get_location(address):\n",
    "#    geocode = dict(*gmaps.geocode([address]))['address_components']\n",
    "#    location = str(geocode[3]['long_name'])\n",
    "#    \n",
    "#    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants = pd.read_pickle('20210205_dataframe_of_4830_restaurants.pickle')\n",
    "#addresses = df_restaurants['Dirección'].to_list()\n",
    "#locations = []\n",
    "#\n",
    "#for address in tqdm(addresses):\n",
    "#    try:\n",
    "#        location = get_location(address)\n",
    "#        locations.append(location)\n",
    "#        \n",
    "#    except:\n",
    "#        locations.append(address)\n",
    "        \n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants['Comuna'] = locations\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants_with_locations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "browsers = utils.gen_browsers(client, geckodriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(url, browsers):\n",
    "    try:\n",
    "        html = requests.get(url['scraping'], timeout=600)\n",
    "    \n",
    "    except Exception as e:\n",
    "        dict_reviews = {'id':hash(url['identifier']), 'restaurant':'timeout', 'grade':'timeout',\n",
    "                        'date_review':'timeout', 'comments':'timeout', 'date_stayed':'timeout',\n",
    "                        'response_body':'timeout', 'user_name':'timeout', 'user_reviews':'timeout',\n",
    "                        'useful_votes':'timeout', 'url':url}\n",
    "        \n",
    "        return dict_reviews\n",
    "        \n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "    soup_reviews = soup.find_all('div', class_ = 'reviewSelector')\n",
    "    dict_months = {'enero':1, 'febrero':2, 'marzo':3, 'abril':4,\n",
    "                  'mayo':5, 'junio':6, 'julio':7, 'agosto':8,\n",
    "                  'septiembre':9, 'octubre':10, 'noviembre':11, 'diciembre':12}\n",
    "\n",
    "    dict_reviews = {'id':[], 'restaurant':[], 'grade':[], 'date_review':[], 'comments':[],\n",
    "                    'date_stayed':[], 'response_body':[], 'user_name':[], 'user_reviews':[],\n",
    "                    'useful_votes':[], 'url':[]}\n",
    "    try:\n",
    "        restaurant = soup.find('h1', class_ = '_3a1XQ88S').text\n",
    "        \n",
    "    except Exception as e:\n",
    "        restaurant = e\n",
    "        \n",
    "    for i, review in enumerate(soup_reviews):\n",
    "        dict_reviews['id'].append(hash(url['identifier']))\n",
    "        dict_reviews['restaurant'].append(restaurant)\n",
    "        grade = str(review.find('div', class_ = 'ui_column is-9').span)\n",
    "        re_grade = int(re.search('_([0-9]+)\">', grade).group(1))\n",
    "        dict_reviews['grade'].append(re_grade)\n",
    "\n",
    "        try:\n",
    "            raw_date = re.search('([0-9]+) de ([a-z]+) de ([0-9]+)',\n",
    "                                 review.find('span', class_ = 'ratingDate').text)\n",
    "\n",
    "            day, month, year = raw_date.group(1), dict_months[raw_date.group(2)], raw_date.group(3)\n",
    "            dict_reviews['date_review'].append('{}/{:02d}/{}'.format(day, month, year))\n",
    "\n",
    "        except:\n",
    "            dict_reviews['date_review'].append(review.find('span', class_ = 'ratingDate').text)\n",
    "        \n",
    "        # In the next lines, we are going to extract the actual reviews.\n",
    "        try:\n",
    "            basic_review = review.find('p', class_ = 'partial_entry').text\n",
    "            extended_review = review.find('span', class_ = 'postSnippet').text\n",
    "            complete_review = basic_review.replace(f'...{extended_review}Más',\n",
    "                                                   f' {extended_review}')\n",
    "        except:\n",
    "            button_code = review.find('span', class_ = 'taLnk ulBlueLinks')\n",
    "                \n",
    "            if (button_code != None) and ('browser' not in locals()):\n",
    "                browser = None\n",
    "                \n",
    "                while browser == None:\n",
    "                    browser = utils.browser_call(browsers)\n",
    "                    if browser == None:\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                browser.driver.get(url['scraping'])                \n",
    "                button = browser.driver.find_element_by_class_name('taLnk.ulBlueLinks').click()\n",
    "                \n",
    "                html_selenium = browser.driver.page_source\n",
    "                soup_selenium = BeautifulSoup(html_selenium, 'lxml')\n",
    "                \n",
    "                reviews_selenium = soup_selenium.find_all('div', class_ = 'reviewSelector')\n",
    "                complete_review = reviews_selenium[i].find('p', class_ = 'partial_entry').text\n",
    "                \n",
    "            else:\n",
    "                complete_review = basic_review\n",
    "                \n",
    "        finally:\n",
    "            complete_review = str(complete_review).replace('\\n', ' ')    \n",
    "            dict_reviews['comments'].append(complete_review)\n",
    "            \n",
    "        # The following lines extract the dates\n",
    "        raw_date = re.search(': ([a-z]+) de ([0-9]+)',\n",
    "                             review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx').text)\n",
    "        try:\n",
    "            month, year = dict_months[raw_date.group(1)], raw_date.group(2)\n",
    "            dict_reviews['date_stayed'].append('{:02d}/{}'.format(month, year))\n",
    "            \n",
    "        except Exception as e:\n",
    "            month, year = review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx'), e\n",
    "            dict_reviews['date_stayed'].append(f'{month} with error: {year}')\n",
    "            \n",
    "\n",
    "        try:\n",
    "            full_response = review.find('div', class_ = 'mgrRspnInline')\n",
    "            local_body = []\n",
    "\n",
    "            for match in ['(.*)\\n', '(.*)\\.\\.\\.Más']:\n",
    "                re_body = re.search(match, full_response.find('p', class_ = 'partial_entry').text)\n",
    "\n",
    "                if re_body != None:\n",
    "                    local_body.append(re_body.group(1)) # Acá agregar marcador para extracción completa\n",
    "                    \n",
    "            dict_reviews['response_body'].append(' '.join(local_body))\n",
    "\n",
    "        except:\n",
    "            full_response = None\n",
    "            dict_reviews['response_body'].append(None)\n",
    "\n",
    "        full_response = review.find('div', class_ = 'entry')\n",
    "        \n",
    "        try:\n",
    "            dict_reviews['user_name'].append(review.find('div', class_ = 'info_text pointer_cursor').text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            dict_reviews['user_name'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e))            \n",
    "        try:\n",
    "            dict_reviews['user_reviews'].append(int(re.match('([0-9]+)',\n",
    "                                                             review.find('span',\n",
    "                                                                         class_ = 'badgeText').text).group(1)))\n",
    "        except Exception as e:\n",
    "            dict_reviews['user_reviews'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e)) \n",
    "\n",
    "        get_votes = lambda useful_votes: n.text if useful_votes != None else 0\n",
    "        dict_reviews['useful_votes'].append(get_votes(review.find('span', class_ = 'numHlpIn')))\n",
    "        \n",
    "        dict_reviews['url'].append(url)\n",
    "        browser.hang()\n",
    "\n",
    "    return dict_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-or10-Volantin-Santiago_Santiago_Metropolitan_Region.html {'id': [8243079183405425641, 8243079183405425641, 8243079183405425641], 'restaurant': ['Volantin', 'Volantin', 'Volantin'], 'grade': [50, 30, 50], 'date_review': ['21/03/2016', '7/09/2019', '18/11/2017'], 'comments': ['El lugar es genial! Lo encontramos de suerte pasando el HBH porque estaba lleno. Tienen musica en vivo casi todos los dias, cerveza, sangria, terremotos, etc. De comida hay papas fritas y los clásicos sandwich. Tienen el formato a lo pobre que es bien requerido por los hombres.', 'No hay una atención muy amable para un bar \"hippie\". En cualquier lugar, muy buena cerveza y buena música en el segundo piso.', 'Qué gloriosa cerveza! Yo personalmente recomiendo este bar a todo el mundo que esté buscando una buena cerveza y buena música. Tienen cueca en vivo cada viernes! A menudo tienen otros música en vivo, y la comida es genial! Patatas fritas rústico son mis favoritas personales. Disfrute!'], 'date_stayed': ['02/2016', '09/2019', '08/2017'], 'response_body': [None, None, None], 'user_name': ['gloriab670', 'siorellana', 'Franco S'], 'user_reviews': [57, 2, 6], 'useful_votes': [0, 0, 0], 'url': [{'identifier': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-Volantin-Santiago_Santiago_Metropolitan_Region.html', 'scraping': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-or10-Volantin-Santiago_Santiago_Metropolitan_Region.html'}, {'identifier': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-Volantin-Santiago_Santiago_Metropolitan_Region.html', 'scraping': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-or10-Volantin-Santiago_Santiago_Metropolitan_Region.html'}, {'identifier': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-Volantin-Santiago_Santiago_Metropolitan_Region.html', 'scraping': 'https://www.tripadvisor.cl/Restaurant_Review-g294305-d10048016-Reviews-or10-Volantin-Santiago_Santiago_Metropolitan_Region.html'}]}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = url_reviews[3]\n",
    "\n",
    "test = get_reviews(url, browsers)\n",
    "print(url['scraping'], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El lugar es genial! Lo encontramos de suerte pasando el HBH porque estaba lleno. Tienen musica en vivo casi todos los dias, cerveza, sangria, terremotos, etc. De comida hay papas fritas y los clásicos sandwich. Tienen el formato a lo pobre que es bien requerido por los hombres. \n",
      "\n",
      "No hay una atención muy amable para un bar \"hippie\". En cualquier lugar, muy buena cerveza y buena música en el segundo piso. \n",
      "\n",
      "Qué gloriosa cerveza! Yo personalmente recomiendo este bar a todo el mundo que esté buscando una buena cerveza y buena música. Tienen cueca en vivo cada viernes! A menudo tienen otros música en vivo, y la comida es genial! Patatas fritas rústico son mis favoritas personales. Disfrute! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for comment in test['comments']:\n",
    "    print(comment, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = url_reviews[7]\n",
    "#\n",
    "#print(url['scraping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
