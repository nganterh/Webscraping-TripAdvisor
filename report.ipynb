{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>WebScraping TripAdvisor</h1>\n",
    "\n",
    "---\n",
    "\n",
    "El código a continuación tiene por objetivo extraer la **[información solicitada](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"Word en Google Drive\")**, desde la página de **[TripAdvisor](https://www.tripadvisor.cl/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html \"Web TripAdvisor\")** para Ximena. La siguiente celda sólo cumple con el propósito de **silenciar las posibles advertencias** que pudieran levantarse al correr el código, pero no aportan mayormente a la comprensión del proceso por parte del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda anterior asegurará que no se desplieguen advertencias innecesarias para la correcta comprensión y lectura de este informe. A continuación se darán las **instrucciones para instalar las librerías** necesarias para correr el código, cuestión que requiere de un comando para ello, por lo que las instrucciones se despliegan como impresión de una celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si es la primera vez que corre este programa, por favor abra la terminal PowerShell de Anaconda e ingrese el siguiente comando: \"\u001b[4mpip install -r C:\\Users\\nicol\\Proyectos\\GitHub\\Webscraping-TripAdvisor\\requirements.txt\u001b[4m\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f'Si es la primera vez que corre este programa, por favor abra la terminal PowerShell de Anaconda' +\n",
    "      f' e ingrese el siguiente comando: \"\\033[4mpip install -r {os.getcwd()}\\\\requirements.txt\\033[4m\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte fundamental de todo programa, corresponde a la **importación de librerías de Python**. Si acaso hubiera errores en esta primera celda, se aconseja contactar a Nicolás Ganter a su correo: nicolas@ganter.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from distributed import Client, LocalCluster\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del código, hay ciertas **variables que es preferible tener en especial consideración**. Entre ellas, encontramos la ubicación del *driver* para *Selenium* que permitirá lanzar una instancia de *Firefox* para navegar la página y extraer los enlaces requeridos en la primera etapa de *webcrawling*. Si aún no ha instalado el driver, acceda a este **[link de descarga](https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip \"geckodriver download link\")**, extraiga el paquete y mueva los documentos a la carpeta de binarios de las librerías de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geckodriver_path = r'C:\\Users\\nicol\\anaconda3\\Library\\bin\\geckodriver'\n",
    "time_id = datetime.today().strftime('%Y%m%d')\n",
    "basic_url = 'https://www.tripadvisor.cl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se creará nuestro clúster para trabajar en forma distribuída cada tarea a realizarse. En particular, utilizaremos un *LocalCluster* para levantar en una máquina cierta cantidad de *workers* con determinada configuración pre-cargada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:62524</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>17.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:62524' processes=6 threads=6, memory=17.02 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCluster(threads_per_worker=1, preload='worker_setup.py')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los restaurantes</h2>\n",
    "La siguiente celda se encargará de la **extracción de los enlaces** asociados a cada restaurante en las páginas especificadas mediante el enlace de la segunda línea. En este caso, se extraerán los restaurantes de Santiago de Chile. Al final de la celda se imprime la cantidad de restaurantes extraídos, la cantidad de restaurantes disponibles según la página, y el porcentaje capturado por el programa. Nótese que el proceso toma algo así como 10 minutos, por lo que se utilizará un atajo mediante *pickles* (estructura de datos propia de este lenguaje de programación) y se especificará la fecha de captura de la información asociada a éste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4830_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Se obtuvieron 4830 restaurantes de 4848 lo que corresponde a una extracción del 99.63%\n",
      "Este proceso tomó 11.43 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url = basic_url + '/Restaurants-g294305-Santiago_Santiago_Metropolitan_Region.html'\n",
    "info = utils.info_restaurants(url, geckodriver_path)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dict_pickles = utils.check_files(dir_files=cwd, keyword='urls')\n",
    "\n",
    "if len(dict_pickles) == 0:\n",
    "    urls = utils.gen_pickle(url, geckodriver_path, info['pages'], basic_url, time_id)\n",
    "\n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_pickles)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        urls = pickle.load(file)\n",
    "    \n",
    "print('Se obtuvieron {} restaurantes de {} lo que corresponde a una extracción del {}%'\n",
    "      .format(len(urls), info['max_restaurants'], round(len(urls) / info['max_restaurants'] * 100, 2)))\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los restaurantes</h2>\n",
    "Con esto concluye la parte más compleja y crítica de la recopilación de enlaces para los restaurantes. No obstante esta tarea continúa luego a nivel de comentarios, **a continuación se procederá a extraer la información solicitada** para cada uno de los restaurantes en la lista. Dado que se utilizan estrategias de computación paralela, no es posible observar el avance, sino abriendo el *Dashboard* cuyo link se encuentra bajo la cuarta celda del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_dataframe_of_4830_restaurants.pickle extraído el 05 de febrero del 2021.\n",
      "Este proceso tomó 0.09 segundos en correr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_dataframes = utils.check_files(dir_files=cwd, keyword='dataframe')\n",
    "\n",
    "if len(dict_dataframes) == 0:\n",
    "    futures = [client.submit(utils.get_restaurant, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_structure = {'id':[], 'Nombre restaurante':[], 'Promedio de calificaciones':[],\n",
    "                      'N° de opiniones':[], 'Calificación de viajeros por categoría':[],\n",
    "                      'Toman medidas de seguridad':[], 'Rankings':[],\n",
    "                      'Tipo de comida y servicios':[], 'url':[]}\n",
    "    \n",
    "    df_restaurants = utils.build_dataframe(dict_structure, results, time_id)\n",
    "    df_restaurants.to_pickle(f'{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle')\n",
    "    print(f'Se guardó \"{time_id}_dataframe_of_{df_restaurants.shape[0]}_restaurants.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_dataframes)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        df_restaurants = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webcrawling de los comentarios</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información cargada del pickle 20210205_4321_review_urls.pickle extraído el 05 de febrero del 2021.\n",
      "Este proceso tomó 0.01 segundos en correr. Se dispone aproximadamente de 43210 comentarios para extraer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='review_urls')\n",
    "\n",
    "if len(dict_files) == 0:\n",
    "    futures = [client.submit(utils.review_urls, url_restaurant) for url_restaurant in list(set(urls))]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_reviews = {key:value for key, value in results if isinstance(value, list)}\n",
    "    n_reviews = len(dict_reviews.values())\n",
    "    \n",
    "    with open(f'{time_id}_{n_reviews}_review_urls.pickle', 'wb') as file:\n",
    "        pickle.dump(dict_reviews, file)\n",
    "    \n",
    "    print(f'Se guardó \"{time_id}_{n_reviews}_review_urls.pickle\" en \"{os.getcwd()}\".')\n",
    "    \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_files)\n",
    "    with open(last_pickle, 'rb') as file:\n",
    "        dict_reviews = pickle.load(file)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "      'Se dispone aproximadamente de {} comentarios para extraer.\\n'.format(len(dict_reviews.values())*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Webscraping de los comentarios</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart = time.time()\\ndict_files = utils.check_files(dir_files=cwd, keyword=\\'scraped_reviews\\')\\nurl_reviews = utils.prepare_urls(dict_reviews)\\n\\nif len(dict_files) == 0:\\n    #browsers = utils.gen_browsers(client, geckodriver_path)\\n    futures = [client.submit(utils.get_reviews, url) for url in url_reviews]\\n    results = client.gather(futures)\\n    \\n    dict_structure = {\\'id\\':[], \\'date_review\\':[], \\'comments\\':[], \\'date_stayed\\':[], \\'response_body\\':[],\\n                      \\'user_name\\':[], \\'user_reviews\\':[], \\'useful_votes\\':[]}\\n\\n    df_reviews = utils.build_dataframe(dict_structure, results, time_id)\\n    df_pathname = f\\'{time_id}_dataframe_of_{df_reviews.shape[0]}_scraped_reviews.pickle\\'\\n\\n    df_reviews.to_pickle(df_pathname)\\n    print(f\\'Se guardó \"{df_pathname}\" en \"{os.getcwd()}\"\\')\\n        \\nelse:\\n    last_pickle = utils.last_pickle(dict_files)\\n    df_reviews = pd.read_pickle(last_pickle)\\n\\nstop = time.time()\\nprint(f\\'Este proceso tomó {round(stop-start, 2)} segundos en correr.\\',\\n      \\'Se extrajeron {} comentarios.\\n\\'.format(df_reviews.shape[0]))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "dict_files = utils.check_files(dir_files=cwd, keyword='scraped_reviews')\n",
    "url_reviews = utils.prepare_urls(dict_reviews)\n",
    "\n",
    "if len(dict_files) == 0:\n",
    "    #browsers = utils.gen_browsers(client, geckodriver_path)\n",
    "    futures = [client.submit(utils.get_reviews, url) for url in url_reviews]\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "    dict_structure = {'id':[], 'date_review':[], 'comments':[], 'date_stayed':[], 'response_body':[],\n",
    "                      'user_name':[], 'user_reviews':[], 'useful_votes':[]}\n",
    "\n",
    "    df_reviews = utils.build_dataframe(dict_structure, results, time_id)\n",
    "    df_pathname = f'{time_id}_dataframe_of_{df_reviews.shape[0]}_scraped_reviews.pickle'\n",
    "\n",
    "    df_reviews.to_pickle(df_pathname)\n",
    "    print(f'Se guardó \"{df_pathname}\" en \"{os.getcwd()}\"')\n",
    "        \n",
    "else:\n",
    "    last_pickle = utils.last_pickle(dict_files)\n",
    "    df_reviews = pd.read_pickle(last_pickle)\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "      'Se extrajeron {} comentarios.\\n'.format(df_reviews.shape[0]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_files = utils.check_files(dir_files=cwd, keyword='scraped_reviews')\n",
    "url_reviews = utils.prepare_urls(dict_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def get_reviews(url):\n",
    "    try:\n",
    "        html = requests.get(url['scraping'], timeout=600)\n",
    "    \n",
    "    except Exception as e:\n",
    "        dict_reviews = {'id':hash(url['identifier']), 'restaurant':'timeout', 'grade':'timeout',\n",
    "                        'date_review':'timeout', 'comments':'timeout', 'date_stayed':'timeout',\n",
    "                        'response_body':'timeout', 'user_name':'timeout', 'user_reviews':'timeout',\n",
    "                        'useful_votes':'timeout', 'url':url, 'Error':e}\n",
    "        \n",
    "        return dict_reviews\n",
    "        \n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "    soup_reviews = soup.find_all('div', class_ = 'reviewSelector')\n",
    "    dict_months = {'enero':1, 'febrero':2, 'marzo':3, 'abril':4,\n",
    "                  'mayo':5, 'junio':6, 'julio':7, 'agosto':8,\n",
    "                  'septiembre':9, 'octubre':10, 'noviembre':11, 'diciembre':12}\n",
    "\n",
    "    dict_reviews = {'id':[], 'restaurant':[], 'grade':[], 'date_review':[], 'comments':[],\n",
    "                    'date_stayed':[], 'response_body':[], 'user_name':[], 'user_reviews':[],\n",
    "                    'useful_votes':[], 'url':[]}\n",
    "    try:\n",
    "        restaurant = soup.find('h1', class_ = '_3a1XQ88S').text\n",
    "        \n",
    "    except Exception as e:\n",
    "        restaurant = e\n",
    "        \n",
    "    for i, review in enumerate(soup_reviews):\n",
    "        dict_reviews['id'].append(hash(url['identifier']))\n",
    "        dict_reviews['restaurant'].append(restaurant)\n",
    "        grade = str(review.find('div', class_ = 'ui_column is-9').span)\n",
    "        re_grade = int(re.search('_([0-9]+)\">', grade).group(1))\n",
    "        dict_reviews['grade'].append(re_grade)\n",
    "\n",
    "        try:\n",
    "            raw_date = re.search('([0-9]+) de ([a-z]+) de ([0-9]+)',\n",
    "                                 review.find('span', class_ = 'ratingDate').text)\n",
    "\n",
    "            day, month, year = raw_date.group(1), dict_months[raw_date.group(2)], raw_date.group(3)\n",
    "            dict_reviews['date_review'].append('{}/{:02d}/{}'.format(day, month, year))\n",
    "\n",
    "        except:\n",
    "            dict_reviews['date_review'].append(review.find('span', class_ = 'ratingDate').text)\n",
    "        \n",
    "        # In the next lines, we are going to extract the actual reviews.\n",
    "        try:\n",
    "            basic_review = review.find('p', class_ = 'partial_entry').text\n",
    "            extended_review = review.find('span', class_ = 'postSnippet').text\n",
    "            complete_review = basic_review.replace(f'...{extended_review}Más',\n",
    "                                                   f' {extended_review}')\n",
    "        except Exception as e:\n",
    "            button_code = review.find('span', class_ = 'taLnk ulBlueLinks')\n",
    "                \n",
    "            if (button_code != None) and ('browser' not in locals()):\n",
    "                browser = None\n",
    "                \n",
    "                #while browser == None:\n",
    "                #    browser = utils.browser_call(browsers)\n",
    "                #    if browser == None:\n",
    "                #        time.sleep(1)\n",
    "                        \n",
    "                get_worker().browser.driver.get(url['scraping'])                \n",
    "                button = get_worker().browser.driver.find_element_by_class_name('taLnk.ulBlueLinks').click()\n",
    "                \n",
    "                html_selenium = get_worker().browser.driver.page_source\n",
    "                soup_selenium = BeautifulSoup(html_selenium, 'lxml')\n",
    "                \n",
    "                reviews_selenium = soup_selenium.find_all('div', class_ = 'reviewSelector')\n",
    "                complete_review = reviews_selenium[i].find('p', class_ = 'partial_entry').text\n",
    "                \n",
    "            else:\n",
    "                complete_review = basic_review\n",
    "                \n",
    "        finally:\n",
    "            complete_review = str(complete_review).replace('\\n', ' ')    \n",
    "            dict_reviews['comments'].append(complete_review)\n",
    "            \n",
    "        # The following lines extract the dates\n",
    "        raw_date = re.search(': ([a-z]+) de ([0-9]+)',\n",
    "                             review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx').text)\n",
    "        try:\n",
    "            month, year = dict_months[raw_date.group(1)], raw_date.group(2)\n",
    "            dict_reviews['date_stayed'].append('{:02d}/{}'.format(month, year))\n",
    "            \n",
    "        except Exception as e:\n",
    "            month, year = review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx'), e\n",
    "            dict_reviews['date_stayed'].append(f'{month} with error: {year}')\n",
    "            \n",
    "\n",
    "        try:\n",
    "            full_response = review.find('div', class_ = 'mgrRspnInline')\n",
    "            local_body = []\n",
    "\n",
    "            for match in ['(.*)\\n', '(.*)\\.\\.\\.Más']:\n",
    "                re_body = re.search(match, full_response.find('p', class_ = 'partial_entry').text)\n",
    "\n",
    "                if re_body != None:\n",
    "                    local_body.append(re_body.group(1)) # Acá agregar marcador para extracción completa\n",
    "                    \n",
    "            dict_reviews['response_body'].append(' '.join(local_body))\n",
    "\n",
    "        except:\n",
    "            full_response = None\n",
    "            dict_reviews['response_body'].append(None)\n",
    "\n",
    "        full_response = review.find('div', class_ = 'entry')\n",
    "        \n",
    "        try:\n",
    "            dict_reviews['user_name'].append(review.find('div', class_ = 'info_text pointer_cursor').text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            dict_reviews['user_name'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e))            \n",
    "        try:\n",
    "            dict_reviews['user_reviews'].append(int(re.match('([0-9]+)',\n",
    "                                                             review.find('span',\n",
    "                                                                         class_ = 'badgeText').text).group(1)))\n",
    "        except Exception as e:\n",
    "            dict_reviews['user_reviews'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e)) \n",
    "\n",
    "        get_votes = lambda useful_votes: n.text if useful_votes != None else 0\n",
    "        dict_reviews['useful_votes'].append(get_votes(review.find('span', class_ = 'numHlpIn')))\n",
    "        \n",
    "        dict_reviews['url'].append(url)\n",
    "        #browser.hang()\n",
    "\n",
    "    return dict_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'complete_review' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-349c79c8b90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Webscraping\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cancelled\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-75f1d5cc3a5d>\u001b[0m in \u001b[0;36mget_reviews\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mcomplete_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_review\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mdict_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_review\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'complete_review' referenced before assignment"
     ]
    }
   ],
   "source": [
    "task = client.submit(get_reviews, url_reviews[3])\n",
    "task.result().result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import get_worker\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def get_reviews(url):\n",
    "    # First, our request to the main url gets protected against possible timeout errors\n",
    "    try:\n",
    "        html = requests.get(url['scraping'], timeout=600)\n",
    "    \n",
    "    except Exception as e:\n",
    "        dict_reviews = {'id':hash(url['identifier']), 'restaurant':'timeout', 'grade':'timeout',\n",
    "                        'date_review':'timeout', 'comments':'timeout', 'date_stayed':'timeout',\n",
    "                        'response_body':'timeout', 'user_name':'timeout', 'user_reviews':'timeout',\n",
    "                        'useful_votes':'timeout', 'url':url, 'Error':e}\n",
    "        \n",
    "        return dict_reviews\n",
    "    \n",
    "    # If we managed to get a response, our html code is converted to a soup and divided by its reviews\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "    soup_reviews = soup.find_all('div', class_ = 'reviewSelector')\n",
    "    \n",
    "    # A dictionary with the equivalent values per month is created to assist with our dates\n",
    "    dict_months = {'enero':1, 'febrero':2, 'marzo':3, 'abril':4,\n",
    "                  'mayo':5, 'junio':6, 'julio':7, 'agosto':8,\n",
    "                  'septiembre':9, 'octubre':10, 'noviembre':11, 'diciembre':12}\n",
    "    \n",
    "    # Our main dictionary is instanced and prepared to be filled through the following for loop\n",
    "    dict_reviews = {'id':[], 'restaurant':[], 'grade':[], 'date_review':[], 'comments':[],\n",
    "                    'date_stayed':[], 'response_body':[], 'user_name':[], 'user_reviews':[],\n",
    "                    'useful_votes':[], 'url':[]}\n",
    "    \n",
    "    # Although rare, some restaurants do not have any name, or do frequently give errors in that field\n",
    "    try:\n",
    "        restaurant = soup.find('h1', class_ = '_3a1XQ88S').text\n",
    "        \n",
    "    except Exception as e:\n",
    "        restaurant = e\n",
    "    \n",
    "    # With everything what's common for all reviews settled, we can iterate through our reviews\n",
    "    for i, review in enumerate(soup_reviews):\n",
    "\n",
    "        dict_reviews['id'].append(hash(url['identifier']))\n",
    "        dict_reviews['restaurant'].append(restaurant)\n",
    "        grade = str(review.find('div', class_ = 'ui_column is-9').span)\n",
    "        re_grade = int(re.search('_([0-9]+)\">', grade).group(1))\n",
    "        dict_reviews['grade'].append(re_grade)\n",
    "\n",
    "        try:\n",
    "            raw_date = re.search('([0-9]+) de ([a-z]+) de ([0-9]+)',\n",
    "                                 review.find('span', class_ = 'ratingDate').text)\n",
    "\n",
    "            day, month, year = raw_date.group(1), dict_months[raw_date.group(2)], raw_date.group(3)\n",
    "            dict_reviews['date_review'].append('{}/{:02d}/{}'.format(day, month, year))\n",
    "\n",
    "        except:\n",
    "            dict_reviews['date_review'].append(review.find('span', class_ = 'ratingDate').text)\n",
    "        \n",
    "        # First we extract our review text by one of the following three ways\n",
    "        try:\n",
    "            # Alternative 1\n",
    "            basic_review = review.find('p', class_ = 'partial_entry').text\n",
    "            extended_review = review.find('span', class_ = 'postSnippet').text\n",
    "            dict_reviews['comments'].append(basic_review.replace(f'...{extended_review}Más',\n",
    "                                                                 f' {extended_review}'))\n",
    "        except:\n",
    "            # There is no guarantee that we'll find our button. We therefore protect ourselves against\n",
    "            try:\n",
    "                button_code = review.find('span', class_ = 'taLnk ulBlueLinks')\n",
    "            except:\n",
    "                button_code = None\n",
    "            \n",
    "            # Alternative 2    \n",
    "            if button_code != None:\n",
    "                get_worker().browser.driver.get(url['scraping'])\n",
    "                button = get_worker().browser.driver.find_element_by_class_name('taLnk.ulBlueLinks').click()\n",
    "                time.sleep(1)\n",
    "\n",
    "                html_selenium = get_worker().browser.driver.page_source\n",
    "                soup_selenium = BeautifulSoup(html_selenium, 'lxml')\n",
    "\n",
    "                reviews_selenium = soup_selenium.find_all('div', class_ = 'reviewSelector')\n",
    "                dict_reviews['comments'].append(reviews_selenium[i].find('p', class_ = 'partial_entry').text)\n",
    "                \n",
    "            # Alternative 3\n",
    "            else:\n",
    "                dict_reviews['comments'].append(basic_review)\n",
    "\n",
    "        # The following lines extract the dates\n",
    "        raw_date = re.search(': ([a-z]+) de ([0-9]+)',\n",
    "                             review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx').text)\n",
    "        try:\n",
    "            month, year = dict_months[raw_date.group(1)], raw_date.group(2)\n",
    "            dict_reviews['date_stayed'].append('{:02d}/{}'.format(month, year))\n",
    "            \n",
    "        except Exception as e:\n",
    "            month, year = review.find('div', class_ = 'prw_rup prw_reviews_stay_date_hsx'), e\n",
    "            dict_reviews['date_stayed'].append(f'{month} with error: {year}')\n",
    "            \n",
    "\n",
    "        try:\n",
    "            full_response = review.find('div', class_ = 'mgrRspnInline')\n",
    "            local_body = []\n",
    "\n",
    "            for match in ['(.*)\\n', '(.*)\\.\\.\\.Más']:\n",
    "                re_body = re.search(match, full_response.find('p', class_ = 'partial_entry').text)\n",
    "\n",
    "                if re_body != None:\n",
    "                    local_body.append(re_body.group(1)) # Acá agregar marcador para extracción completa\n",
    "                    \n",
    "            dict_reviews['response_body'].append(' '.join(local_body))\n",
    "\n",
    "        except:\n",
    "            full_response = None\n",
    "            dict_reviews['response_body'].append(None)\n",
    "\n",
    "        full_response = review.find('div', class_ = 'entry')\n",
    "                \n",
    "        # After extracting our comments, we ... author name\n",
    "        try:\n",
    "            dict_reviews['user_name'].append(review.find('div', class_ = 'info_text pointer_cursor').text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            dict_reviews['user_name'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e)) \n",
    "            \n",
    "        try:\n",
    "            dict_reviews['user_reviews'].append(int(re.match('([0-9]+)',\n",
    "                                                             review.find('span',\n",
    "                                                                         class_ = 'badgeText').text).group(1)))\n",
    "        except Exception as e:\n",
    "            dict_reviews['user_reviews'].append('La url {} presenta un error de tipo {}'.format(url['scraping'], e)) \n",
    "\n",
    "        get_votes = lambda useful_votes: n.text if useful_votes != None else 0\n",
    "        dict_reviews['useful_votes'].append(get_votes(review.find('span', class_ = 'numHlpIn')))\n",
    "        \n",
    "        dict_reviews['url'].append(url)    \n",
    "\n",
    "    return (dict_reviews)\n",
    "\n",
    "\n",
    "urls_test = url_reviews[0:100]\n",
    "\n",
    "futures = [client.submit(get_reviews, url) for url in urls_test]\n",
    "results = client.gather(futures)\n",
    "stop = time.time()\n",
    "\n",
    "dict_structure = {'id':[], 'date_review':[], 'comments':[], 'date_stayed':[], 'response_body':[],\n",
    "                  'user_name':[], 'user_reviews':[], 'useful_votes':[]}\n",
    "\n",
    "df_reviews = utils.build_dataframe(dict_structure, results, time_id)\n",
    "\n",
    "print(f'Este proceso tomó {round(stop-start, 2)} segundos')\n",
    "display(df_reviews)\n",
    "\n",
    "#for i, url in enumerate(url_reviews):\n",
    "#    test = client.submit(search_url, url)\n",
    "#    print(url['scraping'], '\\n', test.result(), '\\n\\n')\n",
    "#    \n",
    "#    if i > 3:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.to_excel('test01.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generación de tablas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants.xlsx')\n",
    "#df_reviews.to_excel(f'{time_id}_excel_with_{df_reviews.shape[0]}_reviews.xlsx')\n",
    "#stop = time.time()\n",
    "#\n",
    "#print(f'Este proceso tomó {round(stop-start, 2)} segundos en correr.',\n",
    "#      'Se extrajeron {} comentarios.\\n'.format(df_reviews.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extraer comunas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import googlemaps\n",
    "#\n",
    "#gmaps = googlemaps.Client(key='AIzaSyAv9kBNSqEznAwQ3nhnb1A6GZPlxJteLE8')\n",
    "#\n",
    "#def get_location(address):\n",
    "#    geocode = dict(*gmaps.geocode([address]))['address_components']\n",
    "#    location = str(geocode[3]['long_name'])\n",
    "#    \n",
    "#    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants = pd.read_pickle('20210205_dataframe_of_4830_restaurants.pickle')\n",
    "#addresses = df_restaurants['Dirección'].to_list()\n",
    "#locations = []\n",
    "#\n",
    "#for address in tqdm(addresses):\n",
    "#    try:\n",
    "#        location = get_location(address)\n",
    "#        locations.append(location)\n",
    "#        \n",
    "#    except:\n",
    "#        locations.append(address)\n",
    "        \n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_restaurants['Comuna'] = locations\n",
    "#df_restaurants.to_excel(f'{time_id}_excel_with_{df_restaurants.shape[0]}_restaurants_with_locations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = url_reviews[7]\n",
    "#\n",
    "#print(url['scraping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
